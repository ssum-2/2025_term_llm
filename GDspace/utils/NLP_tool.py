# NLP_tool.py
from __future__ import annotations
import re
from functools import lru_cache
from itertools import chain
from typing import List, Sequence

import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM  # â˜… missing import fixed
from wtpsplit import SaT
import inspect

# optional â€“ only if GPU ONNX available
try:
    import onnxruntime as ort
except ImportError:  # pragma: no cover â€“ CPUâ€‘only env
    ort = None
import warnings
warnings.filterwarnings("ignore")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Lazy loader â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@lru_cache(maxsize=None)
def _get_summarizer(lang: str = "kr"):
    """
    lang ë³„ (ko | en) í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ìµœì´ˆ 1íšŒë§Œ ë¡œë“œí•´ ìºì‹œ
    """
    if "kr" in lang.lower():
        tok_name, model_name = "psyche/KoT5-summarization", "psyche/KoT5-summarization"
    elif "en" in lang.lower():
        tok_name, model_name = "google/long-t5-tglobal-base", "google/long-t5-tglobal-base"
    else:
        raise ValueError("lang should be 'ko' or 'en'")

    tokenizer = AutoTokenizer.from_pretrained(tok_name)
    model     = AutoModelForSeq2SeqLM.from_pretrained(model_name)

    device = "cuda" if torch.cuda.is_available() else "cpu"
    model  = model.to(device)
    return tokenizer, model
def get_summary(ori_text: str, lang: str = "kr", max_pct_of_ori: int = 50, min_pct_of_ori: int = 10) -> str:
    tokenizer, model = _get_summarizer(lang)

    inputs = tokenizer(ori_text, return_tensors="pt", max_length=1024, truncation=True).to(model.device)

    # í† í° ê¸¸ì´ê°€ ì•„ë‹ˆë¼ â€œë¬¸ì ê¸¸ì´ ë¹„ìœ¨â€ ê¸°ì¤€ì´ë©´ len(ori_text) ì‚¬ìš©
    summary_ids = model.generate(
                                 **inputs,
                                 max_length=int(len(ori_text) * max_pct_of_ori // 100),
                                 min_length=int(len(ori_text) * min_pct_of_ori // 100),
                                 num_beams=5,
                                 no_repeat_ngram_size=3,
                                 early_stopping=True,
                                 )

    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)

def _filter_supported(func, kwargs: dict) -> dict:
    """
    ì „ë‹¬í•˜ë ¤ëŠ” kwargs ì¤‘ func ì‹œê·¸ë‹ˆì²˜ì— ì¡´ì¬í•˜ëŠ” ê²ƒë§Œ ë‚¨ê¹€.
    SaT ë²„ì „ì— ë”°ë¼ ì§€ì› íŒŒë¼ë¯¸í„°ê°€ ë‹¬ë¼ì ¸ë„ TypeErrorê°€ ë‚˜ì§€ ì•Šë„ë¡ í•œë‹¤.
    """
    params = inspect.signature(func).parameters
    return {k: v for k, v in kwargs.items() if k in params}

@lru_cache(maxsize=1)
def _load_sat(
    *,
    checkpoint: str = "sat-12l-sm",
    ort_providers: Sequence[str] = ("CUDAExecutionProvider", "CPUExecutionProvider"),
    lora_path: str | None = None,
    fp16: bool = True,  # reserved â€” may be used in newer wtpsplit versions
) -> SaT:  # noqa: WPS231
    """Return cached SaT instance; GPU ONNX if available."""

    # PyTorch path fallback when onnxruntime not present
    if ort is None:
        return SaT(checkpoint, lora_path=lora_path)

    avail = set(ort.get_available_providers())
    providers = [p for p in ort_providers if p in avail] or ["CPUExecutionProvider"]

    # Build kwargs dynamically to avoid breaking older versions
    common_kw = dict(ort_providers=providers)
    if lora_path:
        common_kw["lora_path"] = lora_path

    # attempt fp16 if the current version supports it
    if fp16:
        try:
            return SaT(checkpoint, **common_kw, use_fp16_weights=True)
        except TypeError:
            pass  # silently fall back

    return SaT(checkpoint, **common_kw)

def _normalize_spacing(txt: str) -> str:
    # í•œê¸€<->ì˜ì–´/ìˆ«ì ë¶™ì–´ ìˆëŠ” ìœ„ì¹˜ì— ê³µë°± ë„£ê¸°
    txt = re.sub(r"([ê°€-í£])(?=[A-Za-z0-9])", r"\1 ", txt)
    txt = re.sub(r"([A-Za-z0-9])(?=[ê°€-í£])", r"\1 ", txt)
    # ì¤‘ë³µ ê³µë°± ì •ë¦¬
    return re.sub(r"\s+", " ", txt).strip()


def split_sentences(
    text: str,
    *,
    language: str = "auto",
    domain: str | None = None,
    max_chars: int = 4096,
    threshold: float = 0.025,
    stride: int = 128,
    block_size: int = 256,
    lookahead: int = 64,
    weighting: str = "hat",
    split_on_input_newlines: bool = False,
) -> List[str]:
    if not text:
        return []

    text = _normalize_spacing(text)

    # newlineâ€‘safe chunking
    chunks, buf, size = [], [], 0
    for para in text.splitlines(keepends=True):
        if size + len(para) > max_chars and buf:
            chunks.append("".join(buf)); buf, size = [], 0
        buf.append(para); size += len(para)
    if buf:
        chunks.append("".join(buf))

    sat = _load_sat()

    raw_kw = dict(
        threshold=threshold,
        stride=stride,
        block_size=block_size,
        lookahead=lookahead,
        weighting=weighting,
        split_on_input_newlines=split_on_input_newlines,
        language=None if language == "auto" else language,
        style_or_domain=domain,
    )
    kw = _filter_supported(sat.split, {k: v for k, v in raw_kw.items() if v is not None})

    sents = chain.from_iterable(sat.split(chunks, **kw))
    return [s.strip() for s in sents if s.strip()]

if __name__ == "__main__":
    messy_example = (
        "ì•ˆë…•í•˜ì„¸ìš”ì˜¤ëŠ˜ë„good morningâ˜€ï¸ì˜¤ëŠ˜ì€2025-05-22ëª©ìš”ì¼ì…ë‹ˆë‹¤BTW"
        "ì´ë²ˆì£¼GDP YoY+3.2%(ì˜ˆìƒ2.9%)â€¦ì¤„ì´ê¸°ìŠ¤í‚¬í•„ìš”ğŸ˜…â€œDon't tell me he saidâ€˜ê´œì°®ì•„â€™ë¼ê³ ?"
        "Mr.Smithâ€”a.k.a.â€˜Dr.Noâ€™â€”arrived at 7:30a.m.(UTC+9);ì¡°ì‹ì€ê¹€ì¹˜ë³¶ìŒë°¥ğŸš"
        "ê·¸ëŸ¬ë‚˜è‹±ê²½ìŸì‚¬ë“¤ì€â€˜AI-firstâ€™ì „ëµì„æ¡ç”¨ä¸­, e.g.DeepMind,B.A.I.,Ï„-Labsë“±ë“±."
        "\n\n"
        "â‘ è³‡æœ¬ì˜ç§»å‹•ì†ë„>æ”¿ì˜ì¡°ì •ì†ë„â†’ê·œì œLagë°œìƒ!â‘¡é‡‘åˆ©â†“â†’â€˜ìœ„ê¸°?â€™orâ€˜ê¸°íšŒ?â€™(ç­”ì—†ìŒ)"
        "2024Q4ë³´ê³ ì„œPDF: https://example.com/report.pdf (p.12ì°¸ì¡°)"
        "ì§€ë¦¬ì‚°(1,915m)ëˆˆê½ƒì€>Everest base-campì˜ìƒç¾âœ¨ #mountains #ì„¤ê²½"
        "\n"
        "ëìœ¼ë¡œæ—¥æœ¬èªë„æ··ãœã¦ã¿ã¾ã™ã­ã€‚æ¬¡ã®è¡Œã«ã¯ä¸­æ–‡ä¹Ÿæœ‰ï¼šä»Šå¤©å¤©æ°”ä¸é”™ä½†PM2.5=135Î¼g/mÂ³ğŸ˜­"
        "æœ€å¾Œã«ã‚¹ãƒšãƒ¼ã‚¹ì—†ì´í•œì˜ì¤‘ì¼ëª¨ë‘æ··åœ¨ë!"
    )
    exp = split_sentences(messy_example)
    for s in exp:
        print("â€¢", s)