# pdf_parser.py
import io
import os
import time
import json
import requests
import PyPDF2
import hashlib
import base64
import re
from PIL import Image
from pathlib import Path
import markdown as md
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
from utils.Gemma_tool import get_Gemma_response, clean_gemma_output
from utils.NLP_tool import split_sentences
from typing import List, Dict, Any, Optional, Tuple, Union

# ë¬¸ì„œ ìš”ì†Œ í´ë˜ìŠ¤
class Element:
    def __init__(
        self,
        category: str,
        content: str,
        html: str,
        markdown: str,
        page: int,        id: int,
        base64_encoding: Optional[str] = None,
        image_filename: Optional[str] = None,
        coordinates: Optional[dict] = None
    ):
        self.category = category
        self.content = content
        self.html = html
        self.markdown = markdown
        self.page = page
        self.id = id
        self.base64_encoding = base64_encoding
        self.image_filename = image_filename
        self.coordinates = coordinates

        # ìš”ì•½ ê²°ê³¼ ì €ì¥ìš© í•„ë“œ(ì¶”ê°€)
        self.summary: Optional[str] = None     # ìµœì¢… ìš”ì•½(ì´ë¯¸ì§€+ì£¼ë³€ í…ìŠ¤íŠ¸ ì¢…í•©ìš©)
        self.image_desc: Optional[str] = None  # ì´ë¯¸ì§€ ìì²´ ì„¤ëª…(1íšŒì„±)

    def __repr__(self):
        return f"Element(id={self.id}, category='{self.category}', page={self.page})"
class IncrementalPdfSummarizer:
    """
    - PDF íŒŒì‹± ê²°ê³¼ë¡œë¶€í„° 'ì ì§„ì  ìš”ì•½'ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤
    - ì´ë¯¸ì§€(ë„í‘œÂ·ì°¨íŠ¸ ë“±) ìš”ì†ŒëŠ” Gemma_toolì„ í†µí•´ í•œ ë²ˆ ì„¤ëª…ë°›ê³ ,
      ì£¼ë³€ í…ìŠ¤íŠ¸ì™€ í•©ì³ ìµœì¢… ìš”ì•½ìœ¼ë¡œ ë°˜ì˜
    """

    def __init__(
            self,
            max_workers: int = 1,
            verbose: bool = True,
            gemma_model: str = None,  # Gemma_tool ëª¨ë¸ëª… (None ì‹œ defaults)
    ):
        """
        Args:
            max_workers: ì´ë¯¸ì§€ ìš”ì•½ ë“± ë³‘ë ¬ ì²˜ë¦¬ ì‹œ ì‚¬ìš©í•  ìµœëŒ€ ìŠ¤ë ˆë“œ ìˆ˜
            verbose: ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
            text_model: NLP_tool.get_summary() ì—ì„œ ì‚¬ìš©í•  ì–¸ì–´ êµ¬ë¶„
            gemma_model: Gemma_toolì˜ model_id (None ì‹œ Gemma_tool ë‚´ë¶€ default ì‚¬ìš©)
            use_gemma_for_text: True ì‹œ í…ìŠ¤íŠ¸ ìš”ì•½ë„ Gemma_tool.get_Gemma_response() ì‚¬ìš©
                                False ì‹œ NLP_tool.get_summary() ì‚¬ìš©
        """
        self.max_workers = max_workers
        self.verbose = verbose
        self.gemma_model = gemma_model

    def log(self, msg: str):
        if self.verbose:
            print(f"[IncrementalPdfSummarizer] {msg}")

    def _summarize_text(self, text: str) -> str:
        """
        ìˆœìˆ˜ í…ìŠ¤íŠ¸(ë¬¸ì¥ ìˆ˜ì²œì) ìš”ì•½ í•¨ìˆ˜.
        - NLP_tool ë˜ëŠ” Gemma_tool ì¤‘ ì„ íƒì ìœ¼ë¡œ ì‚¬ìš©
        """
        if not text.strip():
            return ""

        # (ìˆ˜ì •ëœ) Gemma_tool í”„ë¡¬í”„íŠ¸
        # - ì›ë¬¸ ê·¸ëŒ€ë¡œ ë³µë¶™X, ì¶”ì¸¡/ìƒì„±X, í•µì‹¬ ìš”ì•½
        # - í•„ìš”í•˜ë‹¤ë©´ ë¬¸ì¥ ìˆ˜ ì œí•œ/í˜•ì‹ ì§€ì‹œ ì¶”ê°€
        prompt = (
                  "<system>\n"
                  "<bos><start_of_turn>system\n"
                  "ë‹¹ì‹ ì€ ì „ë¬¸ ê¸ˆìœµ ë¶„ì„ê°€ ì…ë‹ˆë‹¤. "
                  "ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ê±°ë‚˜ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ë§ê³ , "
                  "íˆ¬ì ì˜ì‚¬ê²°ì •ì— í•„ìš”í•œ í•µì‹¬ë§Œ ê°„ê²°í•˜ê²Œ ì •ë¦¬í•˜ì‹­ì‹œì˜¤.\n"
                  "<end_of_turn>\n"
                  "<start_of_turn>user\n"
                  "ë‹¤ìŒ ì›ë¬¸ì„ í•œêµ­ì–´ë¡œ ìš”ì•½í•˜ì„¸ìš”:\n\n"
                  f"{text}\n"
                  "<end_of_turn>\n"
                  "<start_of_turn>model\n"
                  )
        stt_time = time.time()
        ans = get_Gemma_response(prompt = prompt, model_id = self.gemma_model, stream = False)
        ans = clean_gemma_output(ans)
        print(f'ans response: {ans}')
        print(f'ans response consume {time.time() - stt_time:.2f}s')
        return ans.strip()


    def _describe_image(self, image_path: str) -> str:
        """
        Gemma_toolì„ ì´ìš©í•´ ì´ë¯¸ì§€ì— ëŒ€í•œ 1íšŒì„± ì„¤ëª…(íŒ©íŠ¸ ìœ„ì£¼) ìƒì„±
        """
        if not image_path or not os.path.isfile(image_path):
            return ""

        # ì•„ë˜ PromptëŠ” ì˜ˆì‹œì´ë¯€ë¡œ, ì£¼ì–´ì§„ ê°€ì´ë“œì— ë§ì¶° ìˆ˜ì • ê°€ëŠ¥:
        # "ì ˆëŒ€ë¡œ ì¶”ì¸¡í•˜ê±°ë‚˜ ì¶”ë¡ í•˜ì§€ ë§ ê²ƒ. ì£¼ì–´ì§„ ì •ë³´ë§Œ ì„¤ëª…í•˜ë¼" ë“±ë“±
        prompt = (
                  "<bos><start_of_turn>system\n"
                  "ë‹¹ì‹ ì€ ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ì‚¬ì‹¤ë§Œ ê¸°ìˆ í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤. "
                  "ì¶”ì¸¡Â·ë°°ê²½ì§€ì‹Â·ì˜ê²¬ì„ í¬í•¨í•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
                  "<end_of_turn>\n"
                  "<start_of_turn>user\n"
                  "ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”\n"
                  "<end_of_turn>\n"
                  "<start_of_turn>model\n"
                  )
        stt_time = time.time()
        desc = get_Gemma_response(prompt=prompt, image_path=image_path, model_id=self.gemma_model)
        desc = clean_gemma_output(desc)
        print(f'desc response: {desc}')
        print(f'desc response consume {time.time() - stt_time:.2f}s')
        return desc.strip()

    def _integrate_image_with_context(self, image_desc: str, context_text: str) -> str:
        """
        ì´ë¯¸ì§€ ì„¤ëª… + ì£¼ë³€ í…ìŠ¤íŠ¸ë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢… ì´ë¯¸ì§€ summaryë¥¼ ìƒì„±
        """
        if (not image_desc.strip()) and (not context_text.strip()):
            return ""

        prompt = (
                  "<bos><start_of_turn>system\n"
                  "ì´ë¯¸ì§€ ì„¤ëª…ê³¼ ì£¼ë³€ í…ìŠ¤íŠ¸ë¥¼ ê²°í•©í•´ í•µì‹¬ ì˜ë¯¸ë§Œ ê°„ê²°íˆ ìš”ì•½í•˜ì„¸ìš”. "
                  "ìƒˆë¡œìš´ ì¶”ë¡ ì´ë‚˜ ìƒìƒì€ ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
                  "<end_of_turn>\n"
                  "<start_of_turn>user\n"
                  "ì´ë¯¸ì§€ ì„¤ëª…:\n"
                  f"{image_desc}\n\n"
                  "ì£¼ë³€ í…ìŠ¤íŠ¸:\n"
                  f"{context_text}\n\n"
                  "ë‘ ì •ë³´ë¥¼ í†µí•©í•´ 3ë¬¸ì¥ ì´í•˜ë¡œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n"
                  "<end_of_turn>\n"
                  "<start_of_turn>model\n"
                  )
        stt_time = time.time()
        combined = get_Gemma_response(prompt = prompt, model_id = self.gemma_model, stream = False)
        combined = clean_gemma_output(combined)
        print(f'combined response: {combined}')
        print(f'combined response consume {time.time() - stt_time:.2f}s')
        return combined.strip()

    def _extract_text_by_page(self, elements: List[Element]) -> Dict[int, str]:
        """
        í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ìš”ì†Œë¥¼ í•œë° ëª¨ì•„ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
        """
        page_text_map = {}
        for elem in elements:
            if elem.category not in ["paragraph", "caption", "list", "heading1", "equation", "index"]:
                continue
            p = elem.page
            if p not in page_text_map:
                page_text_map[p] = []
            page_text_map[p].append(elem.content.strip())

        # page_text_map[page] = "ëª¨ë“  í…ìŠ¤íŠ¸"
        for p in page_text_map:
            page_text_map[p] = "\n".join(page_text_map[p]).strip()
        return page_text_map

    def _split_text_half(self, text: str) -> (str, str):
        """
        í…ìŠ¤íŠ¸ë¥¼ ëŒ€ëµ ë°˜ìœ¼ë¡œ ë‚˜ëˆ„ì–´ (ì•ë¶€ë¶„, ë’·ë¶€ë¶„) ë°˜í™˜
        - ê°œëµì  êµ¬í˜„
        """
        lines = split_sentences(text)
        if not lines:
            return (text, "")
        half = len(lines) // 2
        front = "\n".join(lines[:half])
        back = "\n".join(lines[half:])
        return (front, back)

    def _incremental_page_summarization(self, page_text_map: Dict[int, str]) -> Dict[int, str]:
        """
        1) ê° í˜ì´ì§€ ì „ì²´ ìš”ì•½
        2) (pí›„ë°˜ + p+1ì „ë°˜) êµì°¨ ìš”ì•½
        3) í˜ì´ì§€ë³„ ìµœì¢… ìš”ì•½ = (ì „ì²´ ìš”ì•½ + í•´ë‹¹ êµì°¨ ìš”ì•½) ì¬ìš”ì•½
        """
        sorted_pages = sorted(page_text_map.keys())

        # (A) ê° í˜ì´ì§€ 'ì „ì²´ í…ìŠ¤íŠ¸' ìš”ì•½ì„ ë¨¼ì € êµ¬í•´ë‘ 
        entire_summaries = {}
        for p in sorted_pages:
            raw_text = page_text_map[p]
            if raw_text.strip():
                entire_summaries[p] = self._summarize_text(raw_text)
            else:
                entire_summaries[p] = ""

        # (B) êµì°¨ ìš”ì•½: (page i í›„ë°˜ + page i+1 ì „ë°˜)
        cross_summaries = {}
        for i in range(len(sorted_pages) - 1):
            p = sorted_pages[i]
            p_next = sorted_pages[i + 1]

            # í…ìŠ¤íŠ¸ ì ˆë°˜ ë¶„í• 
            p_front, p_back = self._split_text_half(page_text_map[p])
            n_front, n_back = self._split_text_half(page_text_map[p_next])

            cross_chunk = f"{p_back}\n\n{n_front}".strip()
            if cross_chunk:
                # êµì°¨ êµ¬ê°„ ìš”ì•½
                cross_summaries[p] = self._summarize_text(
                    f"ì•„ë˜ ë‚´ìš©ì€ í˜ì´ì§€ {p} í›„ë°˜ë¶€ì™€ í˜ì´ì§€ {p_next} ì´ˆë°˜ë¶€ì…ë‹ˆë‹¤.\n"
                    f"ì—°ê²°ë˜ëŠ” ì¤‘ìš”í•œ ë‚´ìš©ì„ ê°„ëµíˆ ìš”ì•½í•´ ì£¼ì„¸ìš”:\n\n{cross_chunk}"
                )
            else:
                cross_summaries[p] = ""

        # (C) í˜ì´ì§€ë³„ 'ìµœì¢… ìš”ì•½' ê²°ì •
        final_page_summaries = {}
        for idx, p in enumerate(sorted_pages):
            # ê¸°ë³¸ì€ 'ì „ì²´ ìš”ì•½'
            combined = entire_summaries[p]

            # ë§Œì•½ ë‹¤ìŒ í˜ì´ì§€ì™€ì˜ êµì°¨ ìš”ì•½ì´ ìˆë‹¤ë©´ â†’ ê°™ì´ í•©ì³ ì¬ìš”ì•½
            cross_part = cross_summaries.get(p, "")
            if cross_part.strip():
                # ì „ì²´ + êµì°¨ êµ¬ë¬¸ ë¶™ì—¬ì„œ ë‹¤ì‹œ ìš”ì•½
                merged_text = (
                    f"[í˜ì´ì§€ {p} ì „ì²´ ìš”ì•½]\n{combined}\n\n"
                    f"[í˜ì´ì§€ {p} í›„ë°˜+{p + 1} ì „ë°˜ êµì°¨ ìš”ì•½]\n{cross_part}"
                )
                combined = self._summarize_text(merged_text)

            final_page_summaries[p] = combined

        return final_page_summaries

    def _summarize_images(
            self,
            elements: List[Element],
            page_text_map: Dict[int, str],
    ) -> None:
        """
        ì´ë¯¸ì§€(figure, chart, table ë“±)ì— ëŒ€í•´
        1) ì´ë¯¸ì§€ ìì²´ ì„¤ëª… -> elem.image_desc
        2) ì´ë¯¸ì§€ ì£¼ë³€ í…ìŠ¤íŠ¸ì™€ í•©ì³ ìµœì¢… summary -> elem.summary

        - ì£¼ë³€ í…ìŠ¤íŠ¸: "ê°™ì€ í˜ì´ì§€ í…ìŠ¤íŠ¸"ì—ì„œ ì´ë¯¸ì§€ ì¢Œí‘œ ê·¼ì ‘ ë¶€ë¶„ì„ ë°œì·Œí•˜ê±°ë‚˜,
          ê°„ë‹¨íˆ "ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸"ë¥¼ ì‚¬ìš©í•´ë„ ë¨.
        - ì—¬ê¸°ì„œëŠ” ì˜ˆì‹œë¡œ "ì „ì²´ í˜ì´ì§€ í…ìŠ¤íŠ¸"ë¥¼ ì‚¬ìš©. ì‹¤ì œ êµ¬í˜„ì—ì„  coordinatesë¡œ
          ì£¼ë³€ ë¬¸ì¥ë§Œ ë°œì·Œí•  ìˆ˜ë„ ìˆìŒ.
        """
        # 1) ì´ë¯¸ì§€ë“¤ì„ ë³‘ë ¬ë¡œ "ì´ë¯¸ì§€ ìì²´ ì„¤ëª…" ì–»ê¸°
        #    (Gemma_toolì— ë™ì¼í•˜ê²Œ ìš”ì²­)
        images_to_process = [e for e in elements if e.category in ("figure", "chart", "table") and e.image_filename]
        self.log(f"ì´ {len(images_to_process)}ê°œ ì´ë¯¸ì§€ ìš”ì•½(ê¸°ë³¸ ì„¤ëª…) ì‹œì‘")

        def _desc_job(elem: Element) -> (int, str):
            img_desc = self._describe_image(elem.image_filename)
            return (elem.id, img_desc)

        results_map = {}
        future_to_elem = {}  # futureì™€ Elementë¥¼ ë§¤í•‘
        with ThreadPoolExecutor(max_workers=self.max_workers) as exe:
            # 1) ì‘ì—…ë“¤ì„ futures ë¦¬ìŠ¤íŠ¸ì— ë‹´ê³ 
            futures_list = []
            for e in images_to_process:
                fut = exe.submit(_desc_job, e)
                futures_list.append(fut)
                future_to_elem[fut] = e

            # 2) as_completed()ë¥¼ tqdmë¡œ ê°ì‹¸ ì§„í–‰ë„ í‘œì‹œ
            for fut in tqdm(as_completed(futures_list),
                            total=len(futures_list),
                            desc="Image captioning"):
                eid, img_desc = fut.result()
                results_map[eid] = img_desc

        # ì´í›„ results_mapì„ í†µí•´ image_descë¥¼ í• ë‹¹
        for elem in images_to_process:
            elem.image_desc = results_map.get(elem.id, "")
            page_txt = page_text_map.get(elem.page, "")
            final_summary = self._integrate_image_with_context(elem.image_desc, page_txt)
            elem.summary = final_summary  # ìµœì¢… ìš”ì•½
            self.log(f"[Image summary] Element({elem.id}, page={elem.page}) ìš”ì•½ ì™„ë£Œ")

    def summarize_document(self, parsed_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì‹¤ì œë¡œ PDF ì „ì²´ë¥¼ ìš”ì•½í•˜ëŠ” í•µì‹¬ ë©”ì„œë“œ

        parsed_results ì˜ˆì‹œ êµ¬ì¡°:
        {
            "metadata": [...],
            "elements": [Element(...), ...],
            "total_cost": float,
            ...
        }

        Returns:
            {
                "page_summaries": {í˜ì´ì§€ë²ˆí˜¸: ìš”ì•½ë¬¸, ...},
                "elements": [Element(...)],  # image/tableì€ elem.summary í¬í•¨
                "final_summary":  "ë¬¸ì„œ ì „ì²´ ìš”ì•½ë¬¸"
            }
        """
        elements = parsed_results.get("elements", [])
        if not elements:
            return {
                "page_summaries": {},
                "elements": [],
                "final_summary": ""
            }

        # (1) í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        page_text_map = self._extract_text_by_page(elements)

        # (2) **ì´ë¯¸ì§€ ìš”ì•½ì„ ë¨¼ì € ìˆ˜í–‰** (ì´ë¯¸ì§€ ìº¡ì…”ë‹ + ì£¼ë³€ í…ìŠ¤íŠ¸ ì¢…í•©)
        self._summarize_images(elements, page_text_map)

        if not page_text_map:
            # í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ëŠ” ê²½ìš°(ì´ë¯¸ì§€ë§Œ ìˆëŠ” ë¬¸ì„œ ë“±)ë¼ë©´, ì—¬ê¸°ì„œ ì¢…ë£Œ
            return {
                "page_summaries": {},
                "elements": elements,
                "final_summary": "í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ë¬¸ì„œì…ë‹ˆë‹¤."
            }

        # (3) **í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ì ì§„ ìš”ì•½** (â€œ1í˜ì´ì§€ ì „ì²´ â†’ (1í›„ë°˜+2ì „ë°˜) â†’ 2í˜ì´ì§€ ì „ì²´ â†’ â€¦â€)
        page_summaries = self._incremental_page_summarization(page_text_map)

        # (4) í˜ì´ì§€ ìš”ì•½ ì „ë¶€ í•©ì³ ìµœì¢… ë¬¸ì„œ ìš”ì•½
        all_pages_summary_text = "\n\n".join([
            f"[Page {p}] {page_summaries[p]}" for p in sorted(page_summaries.keys())
        ])
        final_summary = self._summarize_text(all_pages_summary_text)

        # (5) ê²°ê³¼ ë¦¬í„´
        return {
            "page_summaries": page_summaries,
            "elements": elements,  # ì´ë¯¸ì§€ elem.summary ë“± ë°˜ì˜ë¨
            "final_summary": final_summary
        }

# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ë¥¼ íŒŒì¼ë¡œ ì €ì¥
def save_image_from_base64(base64_data: str, output_dir: str, category: str, page: int, index: int) -> Optional[str]:
    """
    base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ë¥¼ PNG íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        base64_data: base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ë°ì´í„°
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        category: ì´ë¯¸ì§€ ì¹´í…Œê³ ë¦¬ (figure, chart, table ë“±)
        page: í˜ì´ì§€ ë²ˆí˜¸
        index: ì¸ë±ìŠ¤ ë²ˆí˜¸
        
    Returns:
        ì €ì¥ëœ ì´ë¯¸ì§€ íŒŒì¼ì˜ ê²½ë¡œ ë˜ëŠ” ì˜¤ë¥˜ ì‹œ None
    """
    try:
        # base64 ë””ì½”ë”©
        image_data = base64.b64decode(base64_data)

        # ë°”ì´íŠ¸ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
        image = Image.open(io.BytesIO(image_data))

        # output_dir ë‚´ì— images í´ë”ì™€ í•˜ìœ„ ì¹´í…Œê³ ë¦¬ í´ë” ìƒì„±
        image_dir = os.path.join(output_dir, "images", category)
        os.makedirs(image_dir, exist_ok=True)
        
        # ì›ë³¸ í´ë”ëª… ì¶”ì¶œ
        original_name = os.path.basename(output_dir)
        
        # í•´ì‹œ ê¸°ë°˜ì˜ ì§§ì€ íŒŒì¼ëª… ìƒì„±
        hash_obj = hashlib.md5((f"{original_name}_{page}_{index}").encode())
        hash_str = hash_obj.hexdigest()[:8]  # 8ì í•´ì‹œê°’ë§Œ ì‚¬ìš©
        
        # ê°„ë‹¨í•œ íŒŒì¼ëª… ìƒì„±
        short_filename = f"{hash_str}_{category[0]}_{page+1}_{index}.png"
        
        # ì €ì¥ ê²½ë¡œ ìƒì„±
        image_path = os.path.join(image_dir, short_filename)
        
        # ì´ë¯¸ì§€ ì €ì¥
        image.save(image_path)
        return image_path
            
    except Exception as e:
        print(f"[Save Image] ì´ë¯¸ì§€ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return None
def _normalize(text: str) -> str:
    # 1) ë‹¨ë½(ë‘ ì¤„ ì´ìƒ ë„ì›€)ì€ ë³´ì¡´í•˜ê³ 
    text = re.sub(r'\n{2,}', '\n\n', text)        # ë‘ ì¤„ ì´ìƒì€ ê·¸ëŒ€ë¡œ
    # 2) ë‚˜ë¨¸ì§€ 1ì¤„ ê°œí–‰ì€ ê³µë°±ìœ¼ë¡œ
    text = re.sub(r'\n(?!\n)', ' ', text)

    # 3) ê³µë°±ì´ ìˆë“  ì—†ë“ , Â·ã†â€¢âˆ™ ë’¤ì— ê³µë°±ì´ ì˜¤ë©´ => ìƒˆ ì¤„ + '* '
    text = re.sub(r'\s*[Â·â€¢âˆ™]\s+', '\n* ', text)   # âœ¨ ë°”ë€ ë¶€ë¶„
    return text.strip()

class DocumentProcessor:
    """
    PDF ë¬¸ì„œë¥¼ ë¶„ì„í•˜ì—¬ êµ¬ì¡°í™”ëœ ë°ì´í„°(Element ëª©ë¡)ë¡œ ë³€í™˜í•˜ê³ ,
    Upstage Document Parse APIë¥¼ í†µí•´ OCR/êµ¬ì¡° ì¸ì‹ì„ ìˆ˜í–‰.
    """

    def __init__(self, api_key: str, config: dict, batch_size: int = 100, use_ocr: bool = True, save_images: bool = True, verbose: bool = True):
        """
        Args:
            api_key: Upstage API í‚¤
            config: APIì— ë„˜ê¸¸ ê¸°ë³¸ íŒŒë¼ë¯¸í„° dict
            batch_size: PDF ë¶„í•  ì‹œ í•œ íŒŒì¼ë‹¹ ìµœëŒ€ í˜ì´ì§€ ìˆ˜
            use_ocr: OCR ì‚¬ìš© ì—¬ë¶€
            save_images: ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€
            verbose: ë¡œê¹… ì—¬ë¶€
        """
        self.api_key = api_key
        self.config = config.copy()
        self.config["ocr"] = use_ocr

        self.batch_size = batch_size
        self.save_images = save_images
        self.verbose = verbose
        self.accumulated_cost = 0  # ì¸ì‹ ë¹„ìš©(ì˜ˆì‹œ)

    def log(self, message: str) -> None:
        if self.verbose:
            print(f"[DocumentProcessor] {message}")

    def split_pdf(self, filepath: str) -> List[str]:
        """
        í° PDFë¥¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ PDFë¡œ ë¶„í• .
        """
        output_folder = os.path.splitext(os.path.basename(filepath))[0]
        output_dir = os.path.join(Path(filepath).parent, output_folder)
        os.makedirs(output_dir, exist_ok=True)
        self.log(f"ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±: {output_dir}")

        try:
            with open(filepath, 'rb') as file:
                input_pdf = PyPDF2.PdfReader(file)
                num_pages = len(input_pdf.pages)
                self.log(f"íŒŒì¼ì˜ ì „ì²´ í˜ì´ì§€ ìˆ˜: {num_pages} pages.")

                split_filepaths = []
                for start_page in range(0, num_pages, self.batch_size):
                    end_page = min(start_page + self.batch_size, num_pages) - 1

                    output_filename = f"split_{start_page:04d}_{end_page:04d}.pdf"
                    output_file = os.path.join(output_dir, output_filename)

                    output_pdf = PyPDF2.PdfWriter()
                    for page_num in range(start_page, end_page + 1):
                        output_pdf.add_page(input_pdf.pages[page_num])

                    with open(output_file, 'wb') as ofs:
                        output_pdf.write(ofs)

                    split_filepaths.append(output_file)
                    self.log(
                        f"ë¶„í•  PDF ìƒì„± ì™„ë£Œ: {os.path.basename(output_file)} "
                        f"(í˜ì´ì§€ {start_page}-{end_page})"
                    )

            return split_filepaths

        except Exception as e:
            self.log(f"PDF ë¶„í•  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            return []

    def parse_start_end_page(self, filepath: str) -> Tuple[int, int]:
        """
        split_0000_0009.pdf ê°™ì€ íŒŒì¼ëª…ì—ì„œ (start_page, end_page) ì¶”ì¶œ
        """
        filename = os.path.basename(filepath)
        name_without_ext = os.path.splitext(filename)[0]
        if not name_without_ext.startswith("split_") or len(name_without_ext) < 15:
            return (-1, -1)

        page_numbers = name_without_ext[6:]  # 0000_0009
        if (
                len(page_numbers) == 9
                and page_numbers[4] == "_"
                and page_numbers[:4].isdigit()
                and page_numbers[5:].isdigit()
        ):
            start_page = int(page_numbers[:4])
            end_page = int(page_numbers[5:])
            if start_page <= end_page:
                return (start_page, end_page)
        return (-1, -1)

    def analyze_document(self, filepath: str) -> Dict[str, Any]:
        """
        Upstage ë¬¸ì„œ ë¶„ì„ API í˜¸ì¶œ í›„ ê²°ê³¼(json) ë°˜í™˜
        """
        # [ì¶”ê°€] ë¨¼ì € ê¸°ì¡´ì— ë¶„ì„ëœ JSON ê²°ê³¼ê°€ ìˆìœ¼ë©´ ë°”ë¡œ ë¡œë“œ
        json_path = os.path.splitext(filepath)[0] + ".json"
        if os.path.exists(json_path):
            self.log(f"ì´ë¯¸ ë¶„ì„ëœ ê²°ê³¼(JSON)ë¥¼ ë°œê²¬: {json_path} â†’ API í˜¸ì¶œ ìƒëµ")
            with open(json_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return data

        headers = {"Authorization": f"Bearer {self.api_key}"}
        try:
            with open(filepath, "rb") as f:
                files = {"document": f}
                response = requests.post(
                    "https://api.upstage.ai/v1/document-ai/document-parse",
                    headers=headers,
                    data=self.config,  # OCR/ì¢Œí‘œ/ì¶œë ¥í˜•ì‹ ë“±
                    files=files,
                )

            if response.status_code == 200:
                data = response.json()

                # .json íŒŒì¼ë¡œë„ ì €ì¥
                output_file = os.path.splitext(filepath)[0] + ".json"
                with open(output_file, "w", encoding="utf-8") as wf:
                    json.dump(data, wf, ensure_ascii=False)

                return data
            else:
                raise ValueError(
                    f"API ìš”ì²­ ì‹¤íŒ¨ [{response.status_code}]: {response.text}"
                )

        except Exception as e:
            raise ValueError(f"ë¬¸ì„œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    def parse_documents(self, filepaths: List[str]) -> Dict[str, Any]:
        """
        ì—¬ëŸ¬ PDF split íŒŒì¼ë“¤ì— ëŒ€í•´ API ë¶„ì„ ìˆ˜í–‰ â†’ ì „ì²´ Element í†µí•©
        """
        if not filepaths:
            self.log("ë¶„ì„í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {"metadata": [], "elements": [], "total_cost": 0}

        self.log(f"ì´ {len(filepaths)}ê°œ íŒŒì¼ ë¶„ì„ ì‹œì‘")
        all_metadata = []
        all_elements = []
        pages_count = 0

        original_dir = None
        if filepaths:
            original_dir = os.path.dirname(os.path.dirname(filepaths[0]))

        for i, filepath in enumerate(filepaths):
            try:
                t0 = time.time()
                self.log(f"íŒŒì¼ ë¶„ì„ ì¤‘({i + 1}/{len(filepaths)}): {filepath}")

                data = self.analyze_document(filepath)
                start_page, _ = self.parse_start_end_page(filepath)
                page_offset = start_page if start_page != -1 else 0

                # page/ID ë³´ì •
                for e in data["elements"]:
                    e["page"] += page_offset

                usage_info = data.get("usage", {})
                if "pages" in usage_info:
                    pages_count += int(usage_info["pages"])

                all_metadata.append({
                    "filepath": filepath,
                    "api": data.get("api"),
                    "model": data.get("model"),
                    "usage": data.get("usage"),
                })
                all_elements.extend(data["elements"])

                dt = time.time() - t0
                self.log(f"â†’ ì™„ë£Œ: {dt:.2f}ì´ˆ ì†Œìš”")

            except Exception as e:
                self.log(f"[ì˜¤ë¥˜] {filepath}: {str(e)}")
                continue

        # ë¹„ìš© ê³„ì‚° (ì˜ˆì‹œ)
        current_cost = pages_count * 0.01
        self.accumulated_cost += current_cost
        self.log(f"í˜„ì¬ ë¶„ì„ ë¹„ìš©: ${current_cost:.2f}, ëˆ„ì : ${self.accumulated_cost:.2f}")

        processed_elements = []
        id_counter = 0

        # Element ë³€í™˜
        for raw in all_elements:
            cat = raw["category"]
            if cat in ["footnote", "header", "footer"]:
                continue

            image_filename = None
            if (
                    self.save_images
                    and cat in ["figure", "chart", "table"]
                    and "base64_encoding" in raw
            ):
                if original_dir:
                    image_filename = save_image_from_base64(
                        raw["base64_encoding"],
                        original_dir,
                        cat,
                        raw["page"],
                        id_counter
                    )

            elem = None
            if cat == "equation":
                elem = Element(
                    category=cat,
                    content=raw["content"]["markdown"] + "\n",
                    html=raw["content"]["html"],
                    markdown=raw["content"]["markdown"],
                    page=raw["page"],
                    id=id_counter,
                    coordinates=raw.get("coordinates"),
                )
            elif cat in ["table", "figure", "chart"]:
                elem = Element(
                    category=cat,
                    content=raw["content"]["markdown"] + "\n",
                    html=raw["content"]["html"],
                    markdown=raw["content"]["markdown"],
                    base64_encoding=raw.get("base64_encoding"),
                    image_filename=image_filename,
                    page=raw["page"],
                    id=id_counter,
                    coordinates=raw.get("coordinates"),
                )
            elif cat == "heading1":
                elem = Element(
                    category=cat,
                    content=f'# {raw["content"]["text"]}\n',
                    html=raw["content"]["html"],
                    markdown=raw["content"]["markdown"],
                    page=raw["page"],
                    id=id_counter,
                    coordinates=raw.get("coordinates"),
                )
            elif cat in ["caption", "paragraph", "list", "index"]:
                clean_text = _normalize(raw["content"]["text"])
                elem = Element(
                    category=cat,
                    content=clean_text + "\n",
                    html=raw["content"]["html"],
                    markdown=clean_text,
                    page=raw["page"],
                    id=id_counter,
                    coordinates=raw.get("coordinates"),
                )

            if elem is not None:
                processed_elements.append(elem)
                id_counter += 1

        self.log(f"â†’ ë³€í™˜ëœ Element: {len(processed_elements)}ê°œ")

        return {
            "metadata": all_metadata,
            "elements": processed_elements,
            "total_cost": current_cost,
            "accumulated_cost": self.accumulated_cost
        }

    def process_pdf(self, filepath: str) -> Dict[str, Any]:
        """
        PDF â†’ split â†’ parse_documents â†’ í†µí•© ê²°ê³¼ ë°˜í™˜
        """
        self.log(f"PDF ì²˜ë¦¬ ì‹œì‘: {filepath}")
        split_files = self.split_pdf(filepath)
        if not split_files:
            return {"elements": [], "total_cost": 0, "error": "PDF ë¶„í•  ì‹¤íŒ¨"}

        result = self.parse_documents(split_files)
        self.log(f"PDF ì²˜ë¦¬ ì™„ë£Œ: {filepath}")
        return result
    
def _element_sort_key(elem: Element):
    """
    ê°™ì€ í˜ì´ì§€ ì•ˆì—ì„œ ì›ë³¸ì˜ ì‹œê°ì  ìˆœì„œë¥¼ ìµœëŒ€í•œ ë³´ì¡´í•˜ê¸° ìœ„í•œ ì •ë ¬ í‚¤
    1) page â†’ 2) y1 â†’ 3) x1 â†’ 4) id
    ì¢Œí‘œê°€ ì—†ì„ ë•ŒëŠ” idë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    if elem.coordinates and isinstance(elem.coordinates, dict):
        return (
            elem.page,
            elem.coordinates.get("y1", 0),
            elem.coordinates.get("x1", 0),
            elem.id,
        )
    return (elem.page, elem.id)
def reconstruct_to_markdown(elements: List[Element], output_dir: Union[str, Path], filename: str = "reconstructed.md") -> Path:
    """
    Element ê°ì²´ë¥¼ Markdown ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥
    """
    # í˜ì´ì§€ë³„ ì •ë ¬
    elements_sorted = sorted(elements, key=_element_sort_key)

    # í˜ì´ì§€ë³„ë¡œ êµ¬ë¶„ì„  ì¶”ê°€
    md_lines = []
    current_page = None
    for elem in elements_sorted:
        if current_page is None or elem.page != current_page:
            # ìƒˆ í˜ì´ì§€ ì‹œì‘
            if current_page is not None:
                md_lines.append("\n---\n")  # í˜ì´ì§€ êµ¬ë¶„ì„ 
            md_lines.append(f"\n<!-- Page {elem.page + 1} -->\n")
            current_page = elem.page

        # ì´ë¯¸ì§€ â†’ ì´ë¯¸ì§€ íŒŒì¼ ë§í¬, ê·¸ ì™¸ â†’ markdown
        if elem.category in ("figure", "chart", "table") and elem.image_filename:
            rel = os.path.relpath(elem.image_filename, output_dir)
            md_lines.append(f"![{elem.category}_{elem.id}]({rel})\n")
            md_lines.append(elem.markdown + "\n")  # í‘œ ìº¡ì…˜/ì„¤ëª… ë“±
        else:
            md_lines.append(elem.markdown + "\n")

    # ì €ì¥
    output_path = Path(output_dir) / filename
    with open(output_path, "w", encoding="utf-8") as f:
        f.writelines(md_lines)

    return output_path
def reconstruct_to_html(elements: List[Element], output_dir: Union[str, Path], filename: str = "reconstructed.html") -> Path:
    """
    Element ê°ì²´ë¥¼ HTML ë¡œ ì¬êµ¬ì„±í•˜ê³  íŒŒì¼ë¡œ ì €ì¥
    """
    elements_sorted = sorted(elements, key=_element_sort_key)

    # ê°„ë‹¨í•œ ìŠ¤íƒ€ì¼ â€“ í˜ì´ì§€ë§ˆë‹¤ êµ¬ë¶„ì„ 
    html_lines = [
        "<!DOCTYPE html>",
        "<html lang='ko'><head>",
        "<meta charset='utf-8'/>",
        "<title>Reconstructed Document</title>",
        "<style>",
        "body{font-family: Pretendard, Arial, sans-serif; line-height:1.6; margin:40px;}",
        ".page{margin-bottom:80px; border-bottom:1px dashed #ccc; padding-bottom:40px;}",
        ".page h1,.page h2,.page h3{margin-top:1.2em;}",
        ".img-wrapper{text-align:center;margin:1em 0;}",
        ".img-wrapper img{max-width:100%;height:auto;border:1px solid #eee;}",
        "</style></head><body>",
    ]

    current_page = None
    for elem in elements_sorted:
        if current_page is None or elem.page != current_page:
            # ìƒˆ í˜ì´ì§€ div ì‹œì‘
            if current_page is not None:
                html_lines.append("</div>")  # ì´ì „ page div ë‹«ê¸°
            html_lines.append(f"<div class='page' id='page-{elem.page + 1}'>")
            html_lines.append(f"<h2 style='margin-top:0'>Page {elem.page + 1}</h2>")
            current_page = elem.page

        # ì½˜í…ì¸  ì‚½ì…
        if elem.category in ("figure", "chart") and elem.image_filename:
            rel = os.path.relpath(elem.image_filename, output_dir)
            html_lines.append("<div class='img-wrapper'>")
            html_lines.append(f"<img src='{rel}' alt='{elem.category}_{elem.id}' />")
            # ì„¤ëª… ìˆëŠ” ê²½ìš°
            if elem.markdown.strip():
                cap_html = md.markdown(elem.markdown)
                html_lines.append(cap_html)
            html_lines.append("</div>")
        elif elem.category == "table":
            # HTML ë²„ì „ì´ ê°€ì¥ ë ˆì´ì•„ì›ƒì´ ì •í™•
            table_html = elem.html if elem.html else md.markdown(elem.markdown)
            html_lines.append(table_html)
        elif elem.category in ("caption", "paragraph", "list", "index"):  # ğŸ”´ ìƒˆ ì¡°ê±´
            content_html = md.markdown(elem.markdown)  # í•­ìƒ markdown â†’ HTML ë³€í™˜
            html_lines.append(content_html)
        else:
            content_html = elem.html if elem.html else md.markdown(elem.markdown)
            html_lines.append(content_html)

    if current_page is not None:
        html_lines.append("</div>")  # ë§ˆì§€ë§‰ page div ë‹«ê¸°
    html_lines.append("</body></html>")

    output_path = Path(output_dir) / filename
    with open(output_path, "w", encoding="utf-8") as f:
        f.writelines(html_lines)

    return output_path

if __name__ == "__main__":
    pdf_path = "./20250228===ëª¨ë“  ê²ƒì€ ê³„íšëŒ€ë¡œ===ìµœë¯¼í•˜,ê°•ì˜í›ˆ===ì‚¼ì„±ì¦ê¶Œ===ê¸°ì—…ë¶„ì„.pdf"
    UPSTAGE_API_KEY = ''

    # ê¸°ë³¸ ì„¤ì • ìƒìˆ˜
    DEFAULT_CONFIG = {
        "ocr": True,  # OCR ì‚¬ìš© ì—¬ë¶€
        "coordinates": True,  # ì¢Œí‘œ ì •ë³´ í¬í•¨ ì—¬ë¶€
        "output_formats": "['html', 'text', 'markdown']",  # ì¶œë ¥ í˜•ì‹
        "model": "document-parse",  # ì‚¬ìš©í•  ëª¨ë¸
        "base64_encoding": "['figure', 'chart', 'table']",  # base64ë¡œ ì¸ì½”ë”©í•  ìš”ì†Œ
    }

    # ë¬¸ì„œ ì²˜ë¦¬ ê°ì²´ ìƒì„±
    processor = DocumentProcessor(
        api_key=UPSTAGE_API_KEY,  # API í‚¤
        config = DEFAULT_CONFIG,
        batch_size=100,  # í•œ ë²ˆì— ì²˜ë¦¬í•  í˜ì´ì§€ ìˆ˜
        use_ocr=True,  # OCR ì‚¬ìš© ì—¬ë¶€
        save_images=True,  # ì´ë¯¸ì§€ ì €ì¥ ì—¬ë¶€
        verbose=True  # ìƒì„¸ ë¡œê·¸ ì¶œë ¥ ì—¬ë¶€
    )

    # PDF íŒŒì¼ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• 
    split_files = processor.split_pdf(pdf_path)

    # ë¶„í• ëœ PDF íŒŒì¼ ë¶„ì„
    parsed_results = processor.parse_documents(split_files)

    # html_path = reconstruct_to_html(parsed_results.get("elements", []), Path(pdf_path).parent)
    # print(f"HTML ì €ì¥ ì™„ë£Œ: {html_path}")

    # 3) Summarizer ìƒì„± í›„ ë¬¸ì„œ ìš”ì•½ ì‹¤í–‰
    self = IncrementalPdfSummarizer(verbose = True)
    summarized_results = self.summarize_document(parsed_results)