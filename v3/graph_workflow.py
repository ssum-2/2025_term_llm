import asyncio, os, re
from typing import TypedDict, List, Dict, Optional
from collections import defaultdict
from langchain_core.documents import Document
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, START, END, MessageGraph

from dependencies import get_openai_client
from vector_db import save_to_vector_db
from utils import PDFSplitter, LayoutAnalyzer, ElementProcessor
from chains import (
    build_extractor_chain, build_entity_summary_chain,
    build_news_summary_chain, build_grand_summary_chain,
    ReportSummary, NewsSummary, EntityAnalysis, GrandSummary
)
from financial import get_financial_timeseries_data_for_llm, FinancialTimeSeriesData
from news_crawler import get_latest_news


class AnalysisState(TypedDict):
    pdf_paths: List[str]
    mode: str
    llm: BaseChatModel
    mini_llm: BaseChatModel

    # PDF íŒŒì‹±ì„ ìœ„í•œ ìƒì„¸ ìƒíƒœ
    split_pdf_paths: Dict[str, List[str]]
    analysis_results: Dict[str, Dict]
    structured_elements: Dict[str, Dict]

    # ìµœì¢… ê²°ê³¼ë¬¼
    all_docs: List[Document]
    detected_stocks: Dict[str, str]
    financial_data: Dict[str, FinancialTimeSeriesData]
    news_articles: Dict[str, List[dict]]
    report_summary: Optional[ReportSummary]
    news_summary: Optional[NewsSummary]
    grand_summary: Optional[GrandSummary]


async def split_pdf_node(state: AnalysisState) -> dict:
    """PDFë¥¼ ì‘ì€ ì¡°ê°ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ë…¸ë“œ"""
    print("--- 1. PDF ë¶„í•  ë…¸ë“œ ì‹œì‘ ---")
    split_pdf_paths = {}
    for path in state["pdf_paths"]:
        split_pdf_paths[path] = PDFSplitter.split(path)
    return {"split_pdf_paths": split_pdf_paths}


async def layout_analysis_node(state: AnalysisState) -> dict:
    """ë¶„í• ëœ PDF ì¡°ê°ë“¤ì˜ ë ˆì´ì•„ì›ƒì„ ë¶„ì„í•˜ëŠ” ë…¸ë“œ"""
    print("--- 2. Layout ë¶„ì„ ë…¸ë“œ ì‹œì‘ ---")
    upstage_api_key = os.getenv("UPSTAGE_API_KEY")
    if not upstage_api_key: raise ValueError("UPSTAGE_API_KEYê°€ .envì— ì—†ìŠµë‹ˆë‹¤.")

    analyzer = LayoutAnalyzer(upstage_api_key)
    analysis_results = {}

    semaphore = asyncio.Semaphore(5)
    async def analyze_file(path):
        # ì„¸ë§ˆí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë™ì‹œ ì‹¤í–‰ ìˆ˜ë¥¼ ì œì–´
        async with semaphore:
            print(f"  -> Layout ë¶„ì„ ì‹œì‘: {os.path.basename(path)}")
            # analyzer.analyzeëŠ” ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ to_threadë¡œ ë¹„ë™ê¸° ì‹¤í–‰
            result = await asyncio.to_thread(analyzer.analyze, path)
            print(f"  <- Layout ë¶„ì„ ì™„ë£Œ: {os.path.basename(path)}")
            return path, result

    tasks = [analyze_file(p) for paths in state["split_pdf_paths"].values() for p in paths]
    results = await asyncio.gather(*tasks)

    for path, result in results:
        if result: analysis_results[path] = result

    # ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²°ê³¼ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶œë ¥
    failed_analyses = [path for path, res in results if not res]
    if failed_analyses:
        print(f"âŒ Layout ë¶„ì„ ì‹¤íŒ¨ ëª©ë¡ ({len(failed_analyses)}ê°œ):")
        for f in failed_analyses:
            print(f"  - {os.path.basename(f)}")

    return {"analysis_results": analysis_results}


async def process_elements_node(state: AnalysisState) -> dict:
    """
    ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ 'í˜ì´ì§€ ë‹¨ìœ„'ë¡œ ìš”ì†Œë¥¼ ê·¸ë£¹í™”í•˜ê³ ,
    ìš”ì•½ ë° ì£¼ì¥ ì¶”ì¶œì„ ìˆ˜í–‰í•˜ì—¬ API í˜¸ì¶œì„ ìµœì†Œí™”í•˜ëŠ” ë…¸ë“œ
    """
    print("--- 3. ìš”ì†Œ ì²˜ë¦¬ ë° ë¬¸ì„œ ìƒì„± ë…¸ë“œ ì‹œì‘ (ë¹„ìš© ìµœì í™” ë²„ì „) ---")
    llm, mini_llm = state['llm'], state['mini_llm']
    openai_client = get_openai_client()
    all_docs = []

    for original_path, split_paths in state["split_pdf_paths"].items():
        source_file = os.path.basename(original_path)
        relevant_analysis = {p: state["analysis_results"][p] for p in split_paths if p in state["analysis_results"]}
        if not relevant_analysis: continue

        page_elements = ElementProcessor.extract_and_structure_elements(relevant_analysis)
        author_match = re.search(r'([\w\s]+(?:ì¦ê¶Œ|íˆ¬ìì¦ê¶Œ|ìì‚°ìš´ìš©|ê²½ì œì—°êµ¬ì†Œ|ë¦¬ì„œì¹˜))',
                                 list(relevant_analysis.values())[0].get('html', ''))
        author = author_match.group(1).strip().replace("\n", " ") if author_match else "ì‘ì„±ì"

        print(f"ğŸ“„ [{source_file}] ì €ì: '{author}', í˜ì´ì§€: {len(page_elements)}")

        ElementProcessor.crop_and_save_elements(original_path, page_elements)

        # --- ìˆ˜ì •ëœ í•µì‹¬ ë¡œì§: í˜ì´ì§€ ë‹¨ìœ„ ì²˜ë¦¬ ---
        page_content_for_extraction = []
        for page_num, elements in page_elements.items():
            # 1. í•œ í˜ì´ì§€ì˜ ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œë¥¼ ê²°í•©
            page_text = "\n".join([e.get('text', '') for e in elements if e.get('category') in ['title', 'header', 'paragraph', 'list']])

            # 2. (ì„ íƒì ) ì´ë¯¸ì§€/í…Œì´ë¸” ìš”ì•½ (ê¸°ì¡´ ë¡œì§ ìœ ì§€í•˜ë˜, ë¹„ìš© ë¬¸ì œ ì‹œ ë¹„í™œì„±í™” ê°€ëŠ¥)
            multimodal_summaries = []
            mm_tasks = [
                asyncio.to_thread(ElementProcessor.summarize_element, e, page_text, mini_llm, openai_client)
                for e in elements if e.get('category') in ['figure', 'table']
            ]
            if mm_tasks:
                summaries = await asyncio.gather(*mm_tasks)
                multimodal_summaries = list(filter(None, summaries))

            # 3. í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€/í…Œì´ë¸” ìš”ì•½ì„ í•©ì³ í˜ì´ì§€ ì „ì²´ ìš”ì•½ë³¸ ìƒì„±
            full_page_content = page_text
            if multimodal_summaries:
                full_page_content += "\n\n[ì´ë¯¸ì§€/í‘œ ìš”ì•½]\n" + "\n".join(multimodal_summaries)

            page_content_for_extraction.append(full_page_content)
            all_docs.append(Document(page_content=full_page_content, metadata={"source_file": source_file, "page": page_num,
                                                                          "element_type": "page_summary"}))

        # --- ì£¼ì¥ ì¶”ì¶œ: ì „ì²´ ë¦¬í¬íŠ¸ ë‚´ìš©ì„ í•œ ë²ˆì— ì²˜ë¦¬ ---
        full_report_summary = "\n\n".join(page_content_for_extraction)
        if full_report_summary:
            extractor_chain = build_extractor_chain(llm)
            try:
                # ë¦¬í¬íŠ¸ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ í•œ ë²ˆì— ë„£ì–´ ì£¼ì¥ ì¶”ì¶œ (API í˜¸ì¶œ 1íšŒ)
                extracted_data = await extractor_chain.ainvoke({"text": full_report_summary, "source_entity_name": author})
                assertions = extracted_data.assertions
                print(f"âœ… [{source_file}] ì£¼ì¥ ì¶”ì¶œ: {len(assertions)}ê°œ")
                for assertion in assertions:
                    all_docs.append(Document(
                        page_content=f"ì£¼ì¥: {assertion.assertion}\nê·¼ê±°: {assertion.evidence}",
                        metadata={"source_file": source_file, "page": "N/A", "element_type": "assertion_info",
                                  "source_entity": author, "subject": assertion.subject,
                                  "sentiment": assertion.sentiment}
                    ))
            except Exception as e:
                print(f"âŒ [{source_file}] ì£¼ì¥ ì¶”ì¶œ ì‹¤íŒ¨: {e}")

    await save_to_vector_db(all_docs)
    return {"all_docs": all_docs}

async def detect_stocks_node(state: AnalysisState) -> dict:
    from main import ticker_cache
    if not ticker_cache: return {"detected_stocks": {}}

    stock_counts = defaultdict(int)
    all_docs = state['all_docs']

    stock_names_sorted = sorted(ticker_cache.keys(), key=len, reverse=True)

    for path in state['pdf_paths']:
        filename = os.path.basename(path)
        for stock_name in stock_names_sorted:
            if stock_name in filename:
                stock_counts[stock_name] += 50

    assertion_docs = [doc for doc in all_docs if doc.metadata.get("element_type") == "assertion_info"]
    for doc in assertion_docs:
        subject = doc.metadata.get('subject', '')
        for stock_name in stock_names_sorted:
            if stock_name in subject:
                stock_counts[stock_name] += 10

    text_docs_content = " ".join(
        [doc.page_content for doc in all_docs if doc.metadata.get("element_type") == "page_summary"])
    temp_text_content = text_docs_content
    for stock_name in stock_names_sorted:
        count = temp_text_content.count(stock_name)
        if count > 0:
            stock_counts[stock_name] += count
            temp_text_content = temp_text_content.replace(stock_name, "")

    valid_counts = {
        name: score for name, score in stock_counts.items()
        if "ì¦ê¶Œ" not in name and "ìì‚°ìš´ìš©" not in name and "íˆ¬ì" not in name and "ê¸ˆìœµ" not in name and score > 0
    }

    if not valid_counts:
        print("ï¸âš ï¸ [Node: detect_stocks] íƒì§€ëœ ì¢…ëª© ì—†ìŒ.")
        return {"detected_stocks": {}}

    top_stocks = sorted(valid_counts.items(), key=lambda item: item[1], reverse=True)[:3]
    detected_stocks = {name: ticker_cache[name] for name, count in top_stocks}
    print(f"âœ… [Node: detect_stocks] ìµœì¢… íƒì§€ëœ ì¢…ëª©: {list(detected_stocks.keys())}")
    return {"detected_stocks": detected_stocks}


async def fetch_external_data_node(state: AnalysisState) -> dict:
    detected_stocks = state.get('detected_stocks', {})
    if not detected_stocks:
        print("ï¸âš ï¸ [Node: fetch_external_data] íƒì§€ëœ ì¢…ëª©ì´ ì—†ì–´ ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return {"financial_data": {}, "news_articles": {}}

    financial_tasks = {name: asyncio.to_thread(get_financial_timeseries_data_for_llm, ticker) for name, ticker in
                       detected_stocks.items()}
    news_tasks = {name: asyncio.to_thread(get_latest_news, name, max_results=5) for name in detected_stocks.keys()}

    financial_results, news_results = await asyncio.gather(
        asyncio.gather(*financial_tasks.values()),
        asyncio.gather(*news_tasks.values())
    )

    financial_data = {list(detected_stocks.keys())[i]: data for i, data in enumerate(financial_results) if
                      data and data.price}
    news_articles = {list(detected_stocks.keys())[i]: news for i, news in enumerate(news_results) if news}

    return {"financial_data": financial_data, "news_articles": news_articles}


async def generate_summaries_node(state: AnalysisState) -> dict:
    mini_llm = state['mini_llm']
    assertion_docs = [doc for doc in state['all_docs'] if doc.metadata.get("element_type") == "assertion_info"]

    report_summary = None
    entity_analyses = []

    if assertion_docs:
        assertions_by_entity = defaultdict(list)
        for doc in assertion_docs:
            entity_name = doc.metadata.get('source_entity', 'ì‘ì„±ì')
            assertions_by_entity[entity_name].append(doc)

        entity_summary_chain = build_entity_summary_chain(mini_llm)
        tasks = []
        for name, docs in assertions_by_entity.items():
            if name == 'ì‘ì„±ì' and len(docs) < 2: continue
            context = "\n".join(f"- {d.page_content.split('ê·¼ê±°:')[0].strip()}" for d in docs)
            tasks.append(entity_summary_chain.ainvoke({"entity_name": name, "claims_context": context}))

        results = await asyncio.gather(*tasks, return_exceptions=True)
        entity_analyses = [res for res in results if isinstance(res, EntityAnalysis)]

    if entity_analyses:
        insight_context = "\n\n".join(f"### {a.entity_name}\n- {a.main_stance}" for a in entity_analyses)
        insight_prompt = PromptTemplate.from_template(
            "ë‹¤ìŒì€ ì—¬ëŸ¬ ë¶„ì„ ì£¼ì²´ë“¤ì˜ í•µì‹¬ ì…ì¥ì…ë‹ˆë‹¤. ì´ë¥¼ ì¢…í•©í•˜ì—¬ ì „ì²´ ì‹œì¥ê³¼ ë¶„ì„ ëŒ€ìƒì— ëŒ€í•œ ìµœì¢… ì¸ì‚¬ì´íŠ¸ë¥¼ 3-4 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n{context}")
        overall_insight = (await (insight_prompt | mini_llm).ainvoke({"context": insight_context})).content
        report_summary = ReportSummary(overall_insight=overall_insight, entity_analyses=entity_analyses)
    else:
        text_summaries = [doc.page_content for doc in state['all_docs'] if
                          doc.metadata.get("element_type") == "page_summary"]
        if text_summaries:
            full_text_context = "\n".join(text_summaries)
            fallback_prompt = PromptTemplate.from_template(
                "ë‹¤ìŒì€ ì—¬ëŸ¬ ì¦ê¶Œì‚¬ ë¦¬í¬íŠ¸ì—ì„œ ì¶”ì¶œëœ í•µì‹¬ ë‚´ìš© ìš”ì•½ë³¸ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ë“¤ì„ ì¢…í•©í•˜ì—¬ ë¦¬í¬íŠ¸ ì „ì²´ì˜ í•µì‹¬ì ì¸ íˆ¬ì ì˜ê²¬ê³¼ ì „ë§ì— ëŒ€í•œ 'ì¢…í•© ì¸ì‚¬ì´íŠ¸'ë¥¼ 3-4 ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n{context}"
            )
            fallback_chain = fallback_prompt | mini_llm
            overall_insight = (await fallback_chain.ainvoke({"context": full_text_context})).content
            report_summary = ReportSummary(overall_insight=overall_insight, entity_analyses=[])
        else:
            report_summary = None

    news_summary = None
    all_news = [article for articles_list in state.get('news_articles', {}).values() for article in articles_list]
    if all_news:
        news_summary_chain = build_news_summary_chain(mini_llm)
        generated_part = await news_summary_chain.ainvoke({"news_articles": all_news})
        news_summary = NewsSummary(summary=generated_part.summary, key_events=generated_part.key_events,
                                   articles=all_news)

    return {"report_summary": report_summary, "news_summary": news_summary}


async def generate_grand_summary_node(state: AnalysisState) -> dict:
    report_summary = state.get('report_summary')
    news_summary = state.get('news_summary')

    if not report_summary or not news_summary:
        return {"grand_summary": None}

    grand_summary_chain = build_grand_summary_chain(state['llm'])
    grand_summary = await grand_summary_chain.ainvoke({
        "report_insight": report_summary.overall_insight,
        "news_summary": news_summary.summary
    })

    return {"grand_summary": grand_summary}


def build_graph():
    """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ê³  ì»´íŒŒì¼í•©ë‹ˆë‹¤."""
    workflow = StateGraph(AnalysisState)

    workflow.add_node("split_pdfs", split_pdf_node)
    workflow.add_node("layout_analysis", layout_analysis_node)
    workflow.add_node("process_elements", process_elements_node)
    workflow.add_node("detect_stocks", detect_stocks_node)
    workflow.add_node("fetch_external_data", fetch_external_data_node)
    workflow.add_node("generate_summaries", generate_summaries_node)
    workflow.add_node("generate_grand_summary", generate_grand_summary_node)

    workflow.set_entry_point("split_pdfs")
    workflow.add_edge("split_pdfs", "layout_analysis")
    workflow.add_edge("layout_analysis", "process_elements")
    workflow.add_edge("process_elements", "detect_stocks")
    workflow.add_edge("detect_stocks", "fetch_external_data")
    workflow.add_edge("fetch_external_data", "generate_summaries")
    workflow.add_edge("generate_summaries", "generate_grand_summary")
    workflow.add_edge("generate_grand_summary", END)

    return workflow.compile()


app = build_graph()
async def run_analysis_workflow(pdf_paths: List[str], mode: str, llm: BaseChatModel, mini_llm: BaseChatModel):
    initial_state = {
        "pdf_paths": pdf_paths,
        "mode": mode,
        "llm": llm,
        "mini_llm": mini_llm,
    }
    return await app.ainvoke(initial_state)
